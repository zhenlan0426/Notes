{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dataset:  gives data at index i\n",
    "2. batch_sampler: gives a batch of index\n",
    "3. collate_fn: combines the batch of data in index into a batch of fixed length tensor,  \n",
    "> indices = next(self.batch_sampler)  \n",
    "> batch = self.collate_fn([self.dataset[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "1. defalut collate_fn in dataload(https://github.com/pytorch/pytorch/blob/2032482eb97f0b650f11fd117791c9bc62125dee/torch/utils/data/_utils/collate.py#L42)\n",
    "<pre><code>def default_collate(batch):\n",
    "        r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "        elem = batch[0]\n",
    "        elem_type = type(elem)\n",
    "        if isinstance(elem, torch.Tensor):\n",
    "            out = None\n",
    "            if torch.utils.data.get_worker_info() is not None:\n",
    "                # If we're in a background process, concatenate directly into a\n",
    "                # shared memory tensor to avoid an extra copy\n",
    "                numel = sum([x.numel() for x in batch])\n",
    "                storage = elem.storage()._new_shared(numel)\n",
    "                out = elem.new(storage)\n",
    "            return torch.stack(batch, 0, out=out)\n",
    "        elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "                and elem_type.__name__ != 'string_':\n",
    "            elem = batch[0]\n",
    "            if elem_type.__name__ == 'ndarray':\n",
    "                # array of string classes and object\n",
    "                if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "                return default_collate([torch.as_tensor(b) for b in batch])\n",
    "            elif elem.shape == ():  # scalars\n",
    "                return torch.as_tensor(batch)\n",
    "        elif isinstance(elem, float):\n",
    "            return torch.tensor(batch, dtype=torch.float64)\n",
    "        elif isinstance(elem, int_classes):\n",
    "            return torch.tensor(batch)\n",
    "        elif isinstance(elem, string_classes):\n",
    "            return batch\n",
    "        elif isinstance(elem, container_abcs.Mapping):\n",
    "            return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "        elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "            return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
    "        elif isinstance(elem, container_abcs.Sequence):\n",
    "            transposed = zip(*batch)\n",
    "            return [default_collate(samples) for samples in transposed]\n",
    "        raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
    "</code></pre>\n",
    "\n",
    "2. pad sequence of different length\n",
    "<pre><code>def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label, length)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    _, labels, lengths = zip(*data)\n",
    "    max_len = max(lengths)\n",
    "    n_ftrs = data[0][0].size(1)\n",
    "    features = torch.zeros((len(data), max_len, n_ftrs))\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    for i in range(len(data)):\n",
    "        j, k = data[i][0].size(0), data[i][0].size(1)\n",
    "        features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
    "    return features.float(), labels.long(), lengths.long()\n",
    "</code></pre>\n",
    "\n",
    "3. create batch index by first bucketing sequence to buckets of similar length.To be used as batch_sampler\n",
    "\n",
    "<pre><code>class BySequenceLengthSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source,  \n",
    "                bucket_boundaries, batch_size=64,):\n",
    "        ind_n_len = []\n",
    "        for i, p in enumerate(data_source):\n",
    "            ind_n_len.append( (i, p.shape[0]) )\n",
    "        self.data_source = data_source\n",
    "        self.ind_n_len = ind_n_len\n",
    "        self.bucket_boundaries = bucket_boundaries\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "        data_buckets = dict()\n",
    "        # where p is the id number and seq_len is the length of this id number.\n",
    "        for p, seq_len in self.ind_n_len:\n",
    "            pid = self.element_to_bucket_id(p,seq_len)\n",
    "            if pid in data_buckets.keys():\n",
    "                data_buckets[pid].append(p)\n",
    "            else:\n",
    "                data_buckets[pid] = [p]\n",
    "        for k in data_buckets.keys():\n",
    "            data_buckets[k] = np.asarray(data_buckets[k])\n",
    "        self.data_buckets = data_buckets\n",
    "       \n",
    "    def __iter__(self):\n",
    "        iter_list = []\n",
    "        for k in self.data_buckets.keys():\n",
    "            np.random.shuffle(self.data_buckets[k])\n",
    "            iter_list += (np.array_split(self.data_buckets[k]\n",
    "                           , int(self.data_buckets[k].shape[0]/self.batch_size)))\n",
    "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
    "        # size\n",
    "        for i in iter_list:\n",
    "            yield i.tolist() # as it was stored in an array\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "   \n",
    "    def element_to_bucket_id(self, x, seq_length):\n",
    "        boundaries = list(self.bucket_boundaries)\n",
    "        buckets_min = [np.iinfo(np.int32).min] + boundaries\n",
    "        buckets_max = boundaries + [np.iinfo(np.int32).max]\n",
    "        conditions_c = np.logical_and(\n",
    "          np.less_equal(buckets_min, seq_length),\n",
    "          np.less(seq_length, buckets_max))\n",
    "        bucket_id = np.min(np.where(conditions_c))\n",
    "        return bucket_id\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
